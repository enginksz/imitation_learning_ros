defaults:
  - _self_
  - algorithm: SAC
  - optional optimised_hyperparameters: null

# General options
seed: 0
steps: 1000000
env: ant
# Training/evaluation/logging hyperparameters
bc_pretraining:
  iterations: 0
  learning_rate: 0.00025
  weight_decay: 0
training:
  start: 1000
  interval: 1
  batch_size: 256
  learning_rate: 0.0003
  weight_decay: 0
evaluation:
  interval: 10000
  episodes: 30
logging:
  interval: 1000  # 0 to disable; logging too frequently can crash plotting/result in a large metrics file
# Agent/reinforcement learning hyperparameters
reinforcement:
  actor:
    hidden_size: 256
    depth: 2
    activation: relu
  critic:
    hidden_size: 256
    depth: 2
    activation: relu
  discount: 0.99
  target_temperature: -1
  polyak_factor: 0.995
memory:
  size: 1000000
# Imitation learning hyperparameters
imitation:
  trajectories: 0  # 0 to keep all trajectories; otherwise select number of trajectories
  subsample: 1  # Note that 20 was default in original GAIL implementation
  state_only: false
  absorbing: true
  mix_expert_data: none
  bc_aux_loss: false
# Miscellaneous options
check_time_usage: false
save_trajectories: false
render: false

hydra:
  job:
    chdir: true  # Change working directory to output directory (default behaviour for Hydra < 1.2)
  run:
    dir: ./outputs/${algorithm}_${env}/${now:%m-%d_%H-%M-%S}  # Timestamp experiments up to second precision
  sweep:
    dir: ./outputs/${algorithm}_${env}_sweeper/${now:%m-%d_%H-%M-%S}
